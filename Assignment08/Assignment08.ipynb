{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is autoencoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "自编码器，autoencoder是一种无监督的学习算法，主要用于数据的降维或者特征的抽取；\n",
    "在深度学习中，autoencoder可用于在训练阶段开始前，确定权重矩阵WW的初始值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "greedy search是找每步最优，得到局部最优；beam search是每步找n个最优，最终得到全局最优"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "重要的就更关注，不重要的就忽略。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "无法联系上下文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "由Bi-LSTM组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer具备rnn的优点，并且并行计算能力强，通过自注意力提取不同词之间的特征，计算效率十分高，也解决了短期记忆的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer normalizaiton是将词向量进行标准化，batch normalization是将词的一个维度进行标准化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "传递每个词的位置信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self-attention：self-attention是自己对自己的权重\n",
    "multi-head attention：由多个self-attention组成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "用GPT进行预训练，再在下游任务中进行后续应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked language model是随机遮盖n个词，在下游任务中预测遮盖住的词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word embedding+position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "用BERT进行预训练，再在下游任务中进行后续应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT、GPT2是单向transformer，BERT是双向transformer。\n",
    "GPT2是GPT的进一步版本，训练数据更大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
